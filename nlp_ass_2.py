# -*- coding: utf-8 -*-
"""NLP_Ass_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/173PFhZjMZmiTOuUUUfJ5UAi0K8APJ1is
"""

pip install tensorflow_probability==0.12.2

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import pipeline
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

generator=pipeline('text-generation',model='gpt2')


text1 = generator('cosmetics', max_length=50)[0]['generated_text']
text2 = generator('computer science' , max_length=50)[0]['generated_text']
text3 = generator('movie', max_length=50)[0]['generated_text']
text4 = generator('study in Germany', max_length=50)[0]['generated_text']

corpus = [text1 + " " + text2 + " " + text3 + " " + text4]

print("Document 1:", text1)
print("Document 2:", text2)
print("Document 3:", text3)
print("Document 4:", text4)

def process_document(document):

    cleaned_text = re.sub(r'[^a-zA-Z\s]', '', document)
    # Normalization
    normalized_text = cleaned_text.lower()
    # Tokenization
    tokens = word_tokenize(normalized_text)
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    # Remove stop words
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in lemmatized_tokens if word not in stop_words]

    # Identify Unique Words
    unique_words = set(filtered_tokens)

    return unique_words

def calculate_tfidf(documents):
    vectorizer=TfidfVectorizer()
    tfidf_matrix=vectorizer.fit_transform(documents)
    print(vectorizer.get_feature_names_out())
    return tfidf_matrix

unique_words = process_document(text1)
print(unique_words)
print('***')
tfidf_matrix=calculate_tfidf(corpus)

print(tfidf_matrix)