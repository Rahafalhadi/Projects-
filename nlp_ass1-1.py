# -*- coding: utf-8 -*-
"""Nlp ass1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xSJ63TnxrYgXIT-W86K_OAa1M8RRe-t7
"""

import requests

url = "https://en.m.wikipedia.org/wiki/Wikipedia#:~:text=Wikipedia%20was%20launched%20on%20January,in%20its%20first%20few%20months."  # Replace with the desired URL

response = requests.get(url)

html = response.text

from bs4 import BeautifulSoup
soup = BeautifulSoup(html, "html.parser")
text = soup.get_text()

"""Cleaning Data"""

import re

#Remove website links and URLs, all non-space characters and characters like ://

cleaned_text = re.sub("(http\S+)", "", text)


#Remove email addresses

cleaned_text = re.sub("([\w\.\-\_]+@[\w\.\-\_]+)", "", text)


#Remove digits

cleaned_text = re.sub("(\d+)", "", text)

#Remove special characters

cleaned_text = re.sub("([^a-zA-Z\s])", "", text)

"""Normalization"""

normalized_text = cleaned_text.lower()

"""Tokenization"""

tokens = normalized_text.split()

"""Lemmatization"""

import nltk
nltk.download('wordnet')
nltk.download('stopwords')

from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

lemmatized_words = [lemmatizer.lemmatize(token) for token in tokens]

"""Remove stop words"""

from nltk.corpus import stopwords

stop_words = set(stopwords.words("english"))

filtered_words = [word for word in lemmatized_words if word not in stop_words]

"""Unique Words"""

unique_words = set(filtered_words)

unique_words